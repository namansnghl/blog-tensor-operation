{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensor Funtions\n",
    "##### *Created by Naman Singhal*\n",
    " Get to know me better with [my Linkedin](https://www.linkedin.com/in/namansnghl/)\n",
    " <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Pytorch\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library,used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab.<br> Pytorch contains a datatype called as 'Tensor'(torch.Tensor) which is a special array. A Tensor is capable of storing both a scalar value and a multidimentional matrix with same data-type. A Tensor can be created from python Data types and can be converted back. In addition these tensors can be used to perform special inbuild functions and mathematical operation.\n",
    "Note: A Tensor is capable of storing matrices beyond 3D\n",
    "\n",
    "\n",
    "### Creating a tensor with pre-defined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [4., 5.],\n",
      "        [6., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#2 dimensional tensor\n",
    "new_tensor = torch.tensor([[2. ,3.],[4. ,5.],[6.,1.]])\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Few funtions which will be discussed in this notebook:\n",
    "- [torch.reshape()](#1)\n",
    "- [torch.tensor.apply_()](#2)\n",
    "- [torch.matmul()](#3)\n",
    "- [torch.unique()](#4)\n",
    "- [torch.stack()](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and other required modules\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='1'></a>Function 1 - torch.reshape(input, shape) → Tensor\n",
    "\n",
    "This function is used to change the dimension(shape/size) of a tensor(matrix). The function returns new tensor and does not modify tensor inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension -  torch.Size([12])\n",
      "New Dimension -  torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working (change this)\n",
    "tensor = torch.tensor([1,2,3,4,5,1,24,4,5,6,7,8])\n",
    "tensor2 = torch.reshape(tensor,(4,3))\n",
    "print('Dimension - ',tensor.size())\n",
    "print('New Dimension - ',tensor2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the number of elements\n",
    "tensor.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor of 12 elements can be reshaped to any dimension `(n x p x l x ...)` such that the elements perfectly fit in the new shape with no extra space or less space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  2,  3,  4,  5],\n",
      "        [ 1, 24,  4,  5,  6,  7],\n",
      "        [ 8,  2,  5,  6,  8,  9]])\n",
      "Dimension -  torch.Size([3, 6])\n",
      "\n",
      "tensor([[[ 2,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 1, 24],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  2],\n",
      "         [ 5,  6],\n",
      "         [ 8,  9]]])\n",
      "New Dimension -  torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "tensor = torch.tensor([2,1,2,3,4,5,1,24,4,5,6,7,8,2,5,6,8,9]).reshape(3,6)\n",
    "tensor2 = torch.reshape(tensor,(3,3,2))\n",
    "print('{}\\nDimension - '.format(tensor),tensor.shape)\n",
    "print('\\n{}\\nNew Dimension - '.format(tensor2),tensor2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mat(3 * 6)` has `18` elements. After reshaping `mat(3 * 3 * 2)` has 18 elements. Hence no space is extra or less in new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  2,  3,  4,  5],\n",
      "        [ 1, 24,  4,  5,  6,  7],\n",
      "        [ 8,  2,  5,  6,  8,  9]])\n",
      "Dimension -  torch.Size([3, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 3, 3]' is invalid for input of size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f446763259a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}\\nDimension - '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtensor2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n{}\\nNew Dimension - '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 3, 3]' is invalid for input of size 18"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "tensor = torch.tensor([2,1,2,3,4,5,1,24,4,5,6,7,8,2,5,6,8,9]).reshape(3,6)\n",
    "print('{}\\nDimension - '.format(tensor),tensor.shape)\n",
    "tensor2 = torch.reshape(tensor,(3,3,3))\n",
    "print('\\n{}\\nNew Dimension - '.format(tensor2),tensor2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code breaks when reshaped matrix has more or less space for elemets. Here `18` elements were tried to fit in a matrix with dimesions `[3,3,3]` which can hold upto `27` elements(creates extra null space for `9` elements)<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2 - tensor.apply_(callable)<a id='2'></a>\n",
    "\n",
    "The `.apply_()` function of Pytorch is similar to the `.apply()` function from pandas. This function is used to perform an operation over all the elements of a tensor. It takes an argument as a callble funtion/lambda function which alters and returns modified values of tensor. `.apply_()` is an inplace function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now declare a 2D Tensor with float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 5., 1., 2.],\n",
      "        [3., 1., 5., 3.],\n",
      "        [7., 5., 8., 3.]])\n"
     ]
    }
   ],
   "source": [
    "#Example - 1\n",
    "tensor = torch.tensor([[3,5,1,2],[3,1,5,3],[7,5,8,3]],dtype=torch.float)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to increment the values of the tensor by '0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2000, 5.2000, 1.2000, 2.2000],\n",
       "        [3.2000, 1.2000, 5.2000, 3.2000],\n",
       "        [7.2000, 5.2000, 8.2000, 3.2000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.apply_(lambda x: (x+0.2))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lambda` function declared return `x + 0.2` i.e. an incremented value of x by 0.2 where x is an element from tensor(matrix).\n",
    "The lambda funtion is then performed of every element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[3, 5, 1, 2],\n",
      "        [3, 1, 5, 3],\n",
      "        [7, 5, 8, 3]])\n",
      "\n",
      "Modified Tensor:\n",
      "tensor([[0, 5, 1, 2],\n",
      "        [0, 1, 5, 0],\n",
      "        [7, 5, 8, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "tensor2 = torch.tensor([[3,5,1,2],[3,1,5,3],[7,5,8,3]])\n",
    "print('Original Tensor:\\n{}\\n'.format(tensor2))\n",
    "\n",
    "def check_three(x):\n",
    "    if x==3:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "tensor2.apply_(check_three)\n",
    "print('Modified Tensor:\\n{}'.format(tensor2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we have created a function `check_three(x)` that simply checks the element `3` and replaces it with `0`.<br>the `.apply_()` takes the funtion and passes elements from the matrix `tensor2` one by one. The funtion returns `0` if the elemnt is `3` else returns the element itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[3, 5, 1, 2],\n",
      "        [3, 1, 5, 3],\n",
      "        [7, 5, 8, 3]])\n",
      "\n",
      "Found 3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type NoneType)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-14a6dd159c1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtensor2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_three\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Modified Tensor:\\n{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required (got type NoneType)"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "tensor2 = torch.tensor([[3,5,1,2],[3,1,5,3],[7,5,8,3]])\n",
    "print('Original Tensor:\\n{}\\n'.format(tensor2))\n",
    "\n",
    "def find_three(x):\n",
    "    if x==3:\n",
    "        print('Found 3')\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "tensor2.apply_(find_three)\n",
    "print('Modified Tensor:\\n{}'.format(tensor2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example I modified `check_three()` to `find_three()` function to demonstrate how `.apply_()` breaks.<br>\n",
    "`find_three()` prints `'Found 3'` if 3 is found and does not return anything. In this case the code breaks as the `.apply_()` method expects a return value for every value it passes in a function (`callable`). `.apply_()` is used to modify values in tensor and if no value is returned it throws error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.apply_()` is a very handy function when playing with data but must be used with caution.<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3 - torch.matmul(input, other, out=None) → Tensor<a id='3'></a>\n",
    "\n",
    "`.matmul()` is a matrix multiplication function for two tensors.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended[(broadcasted)](https://www.youtube.com/watch?v=DEqIiH_bkFc) to its dimension for the purpose of the matrix multiply . After the matrix multiply, the prepended dimension is removed.\n",
    "- If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "- If one argument is >=1D and other is >=2D then a batch matrix multiplication is performed where the lower dimension matrix is brought to a dimension equal to the other matrix byprepending a 1 in dimention. Later the prepended dimension is removed.\n",
    "<br><br>*Example:*<br>\n",
    "Assume the dimensions of 2 matrix as - `[j, 1, n, p]` @ `[k, n, p]`<br>\n",
    "The lower dim `[k, n, p]` is brought to higher dimension by prepending dim(1) -  `[j, 1, n, p]` * `[1, k, n, p]` (broadcasting step)<br>\n",
    "Result:`[j, k, n, p]`<br>\n",
    "*Note: the above matric multiplication is valid as the number of columns `p` in `mat[j, 1, n, p]` is equal to number of rows `p` in `mat[k, n, p]`*<br>*(Recommended reading [Matrix Multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html))*<br><br>\n",
    "*Bonus Note: `@` is a left associated operator introduced to substitute .matmul() funtion and make matrix multiplication easier to read and perform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1:\n",
      "tensor([1, 2, 1])\n",
      "\n",
      "matrix 2:\n",
      "tensor([[3, 2],\n",
      "        [5, 2],\n",
      "        [6, 1]])\n",
      "\n",
      "Matrix Multiplication Result:\n",
      "tensor([19,  7])\n"
     ]
    }
   ],
   "source": [
    "#example 1\n",
    "mat1 = torch.tensor([1,2,1])\n",
    "mat2 = torch.tensor([[3,2],[5,2],[6,1]])\n",
    "\n",
    "print('matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'.format(mat1,mat2))\n",
    "print('Matrix Multiplication Result:\\n{}'.format(torch.matmul(mat1,mat2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "```\n",
    "              [3, 2]\n",
    "[1, 2, 1] *   [5, 2]     =   [1*3 + 2*5 + 6*1, 1*2 + 2*2 + 1*1]     =     [19,7]\n",
    "              [6, 1]\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1:\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "\n",
      "matrix 2:\n",
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20]])\n",
      "\n",
      "Matrix Multiplication Result:\n",
      "tensor([[[ 110,  120,  130,  140,  150],\n",
      "         [ 246,  272,  298,  324,  350],\n",
      "         [ 382,  424,  466,  508,  550]],\n",
      "\n",
      "        [[ 518,  576,  634,  692,  750],\n",
      "         [ 654,  728,  802,  876,  950],\n",
      "         [ 790,  880,  970, 1060, 1150]]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "t1 = torch.tensor([x for x in range(1,25)]).reshape([2,3,4])\n",
    "t2 = torch.tensor([x for x in range(1,21)]).reshape([4,5])\n",
    "print('matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'.format(t1,t2))\n",
    "multiplied_with_matmul = torch.matmul(t1,t2)\n",
    "print('Matrix Multiplication Result:\\n{}'.format(multiplied_with_matmul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martix t1 shape = torch.Size([2, 3, 4])\n",
      "Martix t2 shape = torch.Size([4, 5])\n",
      "Martix Multiplication result shape = torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print('''Martix t1 shape = {}\n",
    "Martix t2 shape = {}\n",
    "Martix Multiplication result shape = {}'''.format(t1.shape,t2.shape,multiplied_with_matmul.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above matrix multiplication `mat[2,3,4]` @ `mat[4,5]`, number of columns `4` is equal to number of rows `4` but the dimensions are not same.<br>\n",
    "`mat[2,3,4]` @ `mat[4,5]`<br>\n",
    "`mat[2,3,4]` @ `mat[1,4,5]`(broadcasting step)<br>\n",
    "Resultant - `mat[2,3,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BONUS\n",
    "multiplied_with_operator = t1 @ t2\n",
    "multiplied_with_matmul == multiplied_with_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence `@` is same as matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation about example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 4]' is invalid for input of size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c277d30178f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 4]' is invalid for input of size 24"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "t1 = torch.tensor([x for x in range(1,25)]).reshape([3,4])\n",
    "t2 = torch.tensor([x for x in range(1,16)]).reshape([3,5])\n",
    "t1 @ t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of columns of 1st tensor is not same as the number of rows of 2nd tensor then multiplication cannot be performed.<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4 - torch.unique(input, sorted=True, return_inverse=False, dim=None)<a id='4'></a>\n",
    "\n",
    "Returns the unique elements of the input tensor.\n",
    "\n",
    "> Parameters\n",
    "- input (Tensor) – the input tensor\n",
    "- sorted (bool) – Whether to sort the unique elements output.\n",
    "- return_inverse (bool) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.\n",
    "- return_counts (bool) – Whether to also return the counts for each unique element.\n",
    "- dim (int) – the dimension to apply unique. If None, the unique of the flattened input is returned. default: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the tensor `[1,3,2,3]` was converted to mentioned data-type i.e. `float` and unique values were returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 5, 7, 9])\n",
      "\n",
      "tensor([[5, 0, 2, 1, 2],\n",
      "        [2, 3, 0, 4, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "tensor = torch.tensor([[9, 1, 3, 2, 3],[3,5,1,7,1]])\n",
    "UniqueNew, loc = torch.unique(tensor, return_inverse=True)\n",
    "print(UniqueNew,loc,sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `return_inverse=True` returned a tensor `loc` that stores indices of the elements of new tensor from the old tensor.<br>\n",
    "`loc[0][0]=5` meaning `tensor[0][0]` is stored as `UniqueNew[5]`<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5 - torch.stack(tensors, dim=0, out=None) → Tensor<a id='5'></a>\n",
    "\n",
    "Concatenates sequence of tensors along a new dimension.\n",
    ">Parameters\n",
    "- tensors – sequence of tensors to concatenate\n",
    "- dim (int)(AKA axis) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)\n",
    "- out (Tensor, optional) – the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - All tensors need to be of the same in same dimension. Stacking is not same as concatenating as stacking means adding over another tensor in new dimension and concatenating is adding in given dimensions<br>\n",
    "So if `A` and `B` are of shape `(3, 4)`, `torch.cat([A, B], dim=0)` will be of shape `(6, 4)` and `torch.stack([A, B], dim=0)` will be of shape `(2, 3, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "matrix 2:\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "\n",
      "matrix1 stacked matrix 2:\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "Dimension\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "tensor1 = torch.tensor([x for x in range(1,7)]).reshape([2,3])\n",
    "tensor2 = torch.tensor([x for x in range(7,13)]).reshape([2,3])\n",
    "print('matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'.format(tensor1,tensor2))\n",
    "concat = torch.stack([tensor1,tensor2],dim=0)\n",
    "print('matrix1 stacked matrix 2:\\n{}'.format(concat))\n",
    "print('Dimension\\n{}'.format(concat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example the two matrices are stacked over each other in a new dimension. `[2,3]` stacked over `[2,3]` resulted in `[2,2,3]` i.e. two `[2,3]` stacked in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1:\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "\n",
      "matrix 2:\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "\n",
      "matrix1 stacked matrix 2:\n",
      "tensor([[[ 1,  2,  3,  1,  2,  3],\n",
      "         [ 4,  5,  6,  4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9,  7,  8,  9],\n",
      "         [10, 11, 12, 10, 11, 12]]])\n",
      "Dimension\n",
      "torch.Size([2, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "tensor1 = torch.tensor([x for x in range(1,13)]).reshape([2,2,3])\n",
    "tensor2 = torch.tensor([x for x in range(1,13)]).reshape([2,2,3])\n",
    "print('matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'.format(tensor1,tensor2))\n",
    "concat = torch.cat([tensor1,tensor2], dim=2)\n",
    "print('matrix1 stacked matrix 2:\\n{}'.format(concat))\n",
    "print('Dimension\\n{}'.format(concat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have stacked the two tensors along `dim = 2` meaning `[n x p x l]` along the `l` axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "matrix 2:\n",
      "tensor([[ 7,  8,  9, 10, 11, 12]])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [1, 6] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-72843db07f11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtensor2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mconcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matrix1 stacked matrix 2:\\n{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dimension\\n{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [1, 6] at entry 1"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "tensor1 = torch.tensor([x for x in range(1,7)]).reshape([2,3])\n",
    "tensor2 = torch.tensor([x for x in range(7,13)]).reshape([1,6])\n",
    "print('matrix 1:\\n{}\\n\\nmatrix 2:\\n{}\\n'.format(tensor1,tensor2))\n",
    "concat = torch.stack([tensor1,tensor2],dim=0)\n",
    "print('matrix1 stacked matrix 2:\\n{}'.format(concat))\n",
    "print('Dimension\\n{}'.format(concat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the code breaks easily as the two tensors have a different dimension. `[2,3]` cannot be stacked over a `[1,6]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always better to use the `.cat()` if dimensions are not same<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This covers all the 5 PyTorch Tensor functions that are very useful when handling real data. To learn more about tensor functions in putorch refer the links in reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "Provide links to your references and other interesting articles about tensors\n",
    "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
